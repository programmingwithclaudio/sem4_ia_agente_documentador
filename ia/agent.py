"""
Sistema de DocumentaciÃ³n con IA V4 - AGENTE CONVERSACIONAL
âœ… Saludo natural y contexto del proyecto
âœ… Personalidad conversacional como experto
âœ… GeneraciÃ³n de README.md profesional
âœ… Selector de carpeta para documentaciÃ³n
âœ… InteracciÃ³n fluida e inteligente
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from dotenv import load_dotenv

import gradio as gr
import numpy as np
import faiss
import redis
import pandas as pd
from sentence_transformers import SentenceTransformer

from openai import OpenAI
import anthropic

load_dotenv()

MODEL_CONFIG = {
    "GPT": {
        "model": "gpt-4o",
        "api_key": os.getenv('OPENAI_API_KEY', ''),
        "max_tokens": 4000
    },
    "Claude": {
        "model": "claude-3-5-sonnet-20241022",
        "api_key": os.getenv('ANTHROPIC_API_KEY', ''),
        "max_tokens": 4000
    },
    "DeepSeek": {
        "model": "deepseek-chat",
        "api_key": os.getenv('DEEPSEEK_API_KEY', ''),
        "base_url": "https://api.deepseek.com",
        "max_tokens": 4000
    }
}

EMBEDDINGS_PATH = Path("datasets/embeddings")
DOCS_OUTPUT_PATH = Path("documentacion_generada")
DOCS_OUTPUT_PATH.mkdir(exist_ok=True)


class DocumentationSystemV4:
    """
    Sistema conversacional que actÃºa como un experto del proyecto
    """
    
    def __init__(self):
        self.clients = {}
        self.embedding_model = None
        self.faiss_index = None
        self.redis_client = None
        self.mapeo_indices = None
        self.conversation_history = []
        
        self.embedding_dim = 384
        self.num_features = 8
        self.total_dim = 392
        
        self.cache_endpoints = None
        self.cache_archivos = None
        self.cache_routers = None
        
        # âœ… NUEVO: Estado conversacional
        self.proyecto_nombre = "API Backend"
        self.saludo_inicial_enviado = False
        self.contexto_mostrado = False
        
        self._initialize_clients()
        self._load_embeddings_v3()
        self._cargar_cache_completo()
        self._analizar_proyecto()  # âœ… NUEVO
    
    def _initialize_clients(self):
        """Inicializa clientes LLM"""
        if MODEL_CONFIG["GPT"]["api_key"]:
            try:
                self.clients["GPT"] = OpenAI(api_key=MODEL_CONFIG["GPT"]["api_key"])
                print("âœ… GPT-4o disponible")
            except Exception as e:
                print(f"âš ï¸ Error GPT: {e}")
        
        if MODEL_CONFIG["Claude"]["api_key"]:
            try:
                self.clients["Claude"] = anthropic.Anthropic(
                    api_key=MODEL_CONFIG["Claude"]["api_key"]
                )
                print("âœ… Claude disponible")
            except Exception as e:
                print(f"âš ï¸ Error Claude: {e}")
        
        if MODEL_CONFIG["DeepSeek"]["api_key"]:
            try:
                self.clients["DeepSeek"] = OpenAI(
                    api_key=MODEL_CONFIG["DeepSeek"]["api_key"],
                    base_url=MODEL_CONFIG["DeepSeek"]["base_url"]
                )
                print("âœ… DeepSeek disponible")
            except Exception as e:
                print(f"âš ï¸ Error DeepSeek: {e}")
    
    def _load_embeddings_v3(self):
        """Carga embeddings"""
        try:
            self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            self.embedding_model.max_seq_length = 512
            print("âœ… Modelo cargado")
            
            index_path = EMBEDDINGS_PATH / "documentacion.index"
            if index_path.exists():
                self.faiss_index = faiss.read_index(str(index_path))
                self.total_dim = self.faiss_index.d
                self.num_features = self.total_dim - self.embedding_dim
                print(f"âœ… FAISS: {self.faiss_index.ntotal} vectores")
            
            self.redis_client = redis.Redis(
                host='localhost', port=6379, db=3, decode_responses=True
            )
            self.redis_client.ping()
            print("âœ… Redis DB3")
            
            mapeo_path = EMBEDDINGS_PATH / "mapeo_indices.json"
            if mapeo_path.exists():
                with open(mapeo_path, 'r', encoding='utf-8') as f:
                    self.mapeo_indices = json.load(f)
                print(f"âœ… Mapeo: {len(self.mapeo_indices)} entradas")
            
        except Exception as e:
            print(f"âš ï¸ Error: {e}")
    
    def _cargar_cache_completo(self):
        """Carga todos los datos en memoria"""
        if not self.redis_client or not self.mapeo_indices:
            return
        
        try:
            print("\nğŸ“¦ Cargando cache...")
            
            self.cache_endpoints = []
            self.cache_archivos = {}
            self.cache_routers = {}
            
            for idx in range(len(self.mapeo_indices)):
                data = self.redis_client.hgetall(f"chunk:{idx}")
                if data:
                    tipo = data.get('tipo', '')
                    
                    if tipo == 'route' or data.get('endpoint') or data.get('endpoint_completo'):
                        endpoint_completo = data.get('endpoint_completo', '')
                        endpoint_base = data.get('endpoint', '')
                        router_prefix = data.get('router_prefix', '')
                        
                        if endpoint_completo:
                            endpoint_final = endpoint_completo
                        elif endpoint_base and router_prefix:
                            endpoint_final = f"{router_prefix}{endpoint_base}".replace('//', '/')
                        else:
                            endpoint_final = endpoint_base
                        
                        if endpoint_final:
                            endpoint_info = {
                                'id': idx,
                                'endpoint': endpoint_final,
                                'metodo': data.get('metodo_http', 'HTTP'),
                                'descripcion': data.get('descripcion', ''),
                                'archivo': data.get('archivo', ''),
                                'elemento': data.get('elemento', ''),
                                'router_padre': data.get('router_padre', ''),
                                'response_model': data.get('response_model', ''),
                                'status_code': data.get('status_code', ''),
                                'codigo': data.get('contenido', '')[:500]
                            }
                            self.cache_endpoints.append(endpoint_info)
                    
                    archivo = data.get('archivo', '')
                    if archivo:
                        if archivo not in self.cache_archivos:
                            self.cache_archivos[archivo] = []
                        self.cache_archivos[archivo].append({
                            'id': idx,
                            'tipo': tipo,
                            'elemento': data.get('elemento', ''),
                            'descripcion': data.get('descripcion', '')[:200]
                        })
                    
                    router_padre = data.get('router_padre', '')
                    if router_padre:
                        if router_padre not in self.cache_routers:
                            self.cache_routers[router_padre] = []
                        self.cache_routers[router_padre].append({
                            'id': idx,
                            'tipo': tipo,
                            'elemento': data.get('elemento', ''),
                            'endpoint': endpoint_final if endpoint_final else ''
                        })
            
            print(f"âœ… Cache: {len(self.cache_endpoints)} endpoints, {len(self.cache_archivos)} archivos")
            
        except Exception as e:
            print(f"âš ï¸ Error: {e}")
    
    def _analizar_proyecto(self):
        """
        âœ… NUEVO: Analiza el proyecto para entender su propÃ³sito
        """
        if not self.cache_archivos:
            return
        
        # Detectar tipo de proyecto por archivos
        archivos = list(self.cache_archivos.keys())
        
        if any('auth' in a.lower() for a in archivos):
            self.proyecto_nombre = "API Backend con AutenticaciÃ³n"
        elif any('user' in a.lower() for a in archivos):
            self.proyecto_nombre = "API de GestiÃ³n de Usuarios"
        elif any('product' in a.lower() for a in archivos):
            self.proyecto_nombre = "API de E-commerce"
        else:
            self.proyecto_nombre = "API Backend FastAPI"
    
    def generar_saludo_inicial(self) -> str:
        """
        âœ… NUEVO: Saludo natural y contextual
        """
        if not self.cache_endpoints:
            return """
Â¡Hola! ğŸ‘‹ 

Soy tu asistente de documentaciÃ³n de cÃ³digo. Estoy aquÃ­ para ayudarte a entender y documentar tu proyecto.

Parece que aÃºn no he cargado datos del proyecto. Â¿PodrÃ­as verificar que los archivos de embeddings estÃ©n correctamente generados?
"""
        
        num_endpoints = len(self.cache_endpoints)
        num_archivos = len(self.cache_archivos)
        routers_principales = sorted(self.cache_routers.keys())[:5]
        
        saludo = f"""
Â¡Hola! ğŸ‘‹ Soy tu asistente de documentaciÃ³n para **{self.proyecto_nombre}**.

He analizado tu cÃ³digo y tengo todo listo. AquÃ­ un resumen de lo que conozco:

ğŸ“Š **Proyecto en nÃºmeros:**
- ğŸŒ **{num_endpoints} endpoints** documentados
- ğŸ“ **{num_archivos} archivos** analizados
- ğŸ“¦ **{len(self.cache_routers)} routers** organizados

ğŸ” **Principales mÃ³dulos:**
{chr(10).join([f"   â€¢ {router}" for router in routers_principales])}

---

ğŸ’¬ **Â¿En quÃ© puedo ayudarte?**

Puedo responder preguntas como:
- *"Â¿QuÃ© endpoints tiene la API?"*
- *"Â¿CÃ³mo funciona el sistema de autenticaciÃ³n?"*
- *"ExplÃ­came el endpoint /users"*
- *"Genera un README.md completo"*

TambiÃ©n puedo generar documentaciÃ³n profesional si lo necesitas. Â¡Pregunta lo que quieras! ğŸš€
"""
        
        return saludo
    
    def detectar_tipo_pregunta(self, pregunta: str) -> str:
        """Detecta el tipo de pregunta"""
        pregunta_lower = pregunta.lower()
        
        # Preguntas de saludo/presentaciÃ³n
        if any(kw in pregunta_lower for kw in ['hola', 'hey', 'buenos dÃ­as', 'buenas tardes', 'quÃ© tal']):
            return 'SALUDO'
        
        # Preguntas sobre generaciÃ³n de documentaciÃ³n
        if any(kw in pregunta_lower for kw in ['readme', 'documentaciÃ³n', 'documenta', 'genera doc']):
            return 'GENERAR_DOC'
        
        # Preguntas que requieren listas completas
        keywords_lista = [
            'endpoints', 'rutas', 'api', 'listar', 'mostrar todos',
            'cuÃ¡ntos', 'quÃ© endpoints', 'quÃ© rutas', 'lista de',
            'archivos', 'mÃ³dulos', 'estructura', 'routers'
        ]
        
        if any(kw in pregunta_lower for kw in keywords_lista):
            return 'LISTA'
        
        # Preguntas sobre cÃ³digo especÃ­fico
        keywords_codigo = [
            'cÃ³mo funciona', 'implementa', 'cÃ³digo de', 'funciÃ³n',
            'clase', 'mÃ©todo', 'explica', 'muestra el cÃ³digo'
        ]
        
        if any(kw in pregunta_lower for kw in keywords_codigo):
            return 'CODIGO'
        
        return 'GENERAL'
    
    def obtener_todos_endpoints(self) -> List[Dict]:
        """Obtiene todos los endpoints del cache"""
        return self.cache_endpoints if self.cache_endpoints else []
    
    def buscar_codigo_semantico(self, query: str, top_k: int = 5) -> List[Dict]:
        """BÃºsqueda semÃ¡ntica"""
        if not self.faiss_index or not self.embedding_model:
            return []
        
        try:
            query_embedding = self.embedding_model.encode(
                [query], convert_to_numpy=True, normalize_embeddings=True
            )
            
            features_dummy = np.zeros((1, self.num_features), dtype='float32')
            query_embedding = np.hstack([query_embedding, features_dummy]).astype('float32')
            query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)
            
            distances, indices = self.faiss_index.search(query_embedding, top_k)
            
            resultados = []
            for idx, dist in zip(indices[0], distances[0]):
                chunk_data = self.redis_client.hgetall(f"chunk:{idx}")
                if chunk_data:
                    endpoint_completo = chunk_data.get('endpoint_completo', '')
                    endpoint_base = chunk_data.get('endpoint', '')
                    router_prefix = chunk_data.get('router_prefix', '')
                    
                    if endpoint_completo:
                        endpoint_final = endpoint_completo
                    elif endpoint_base and router_prefix:
                        endpoint_final = f"{router_prefix}{endpoint_base}".replace('//', '/')
                    else:
                        endpoint_final = endpoint_base
                    
                    resultado = {
                        'id': int(idx),
                        'score': float(dist),
                        'archivo': chunk_data.get('archivo', ''),
                        'tipo': chunk_data.get('tipo', ''),
                        'elemento': chunk_data.get('elemento', ''),
                        'endpoint': endpoint_final,
                        'metodo_http': chunk_data.get('metodo_http', ''),
                        'contenido': chunk_data.get('contenido', ''),
                        'descripcion': chunk_data.get('descripcion', ''),
                        'router_padre': chunk_data.get('router_padre', '')
                    }
                    
                    resultados.append(resultado)
            
            return resultados
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            return []
    
    def generar_contexto_tecnico(self) -> str:
        """Genera contexto tÃ©cnico del proyecto"""
        if not self.cache_endpoints:
            return "No hay datos disponibles"
        
        try:
            routers_map = {}
            for ep in self.cache_endpoints:
                router = ep.get('router_padre', 'main')
                if router not in routers_map:
                    routers_map[router] = []
                routers_map[router].append(ep)
            
            contexto = f"""
ğŸ“Š **DATOS DEL PROYECTO:**
   â€¢ Total de endpoints: {len(self.cache_endpoints)}
   â€¢ Total de archivos: {len(self.cache_archivos)}
   â€¢ Total de routers: {len(self.cache_routers)}

ğŸŒ **ENDPOINTS DE LA API:**

"""
            
            for router, endpoints in sorted(routers_map.items()):
                contexto += f"ğŸ“¦ **{router}**\n"
                for ep in sorted(endpoints, key=lambda x: x['endpoint'])[:20]:
                    contexto += f"   â€¢ {ep['metodo']:7} {ep['endpoint']}\n"
                contexto += "\n"
            
            return contexto
            
        except Exception as e:
            return f"Error: {str(e)}"
    
    def generar_readme_completo(self, carpeta_destino: str) -> Tuple[str, str]:
        """
        âœ… NUEVO: Genera README.md profesional
        """
        if not self.cache_endpoints:
            return "âŒ No hay datos para generar README", ""
        
        try:
            # Agrupar endpoints
            routers_map = {}
            for ep in self.cache_endpoints:
                router = ep.get('router_padre', 'General')
                if router not in routers_map:
                    routers_map[router] = []
                routers_map[router].append(ep)
            
            # Generar contenido README
            readme = f"""# ğŸ“š DocumentaciÃ³n - {self.proyecto_nombre}

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto es una API REST desarrollada con FastAPI que proporciona {len(self.cache_endpoints)} endpoints organizados en {len(self.cache_routers)} routers principales.

## ğŸš€ CaracterÃ­sticas

- âœ… API RESTful con FastAPI
- ğŸ” Sistema de autenticaciÃ³n y autorizaciÃ³n
- ğŸ“Š {len(self.cache_endpoints)} endpoints documentados
- ğŸ—‚ï¸ Organizado en {len(self.cache_routers)} mÃ³dulos

## ğŸ“ Estructura del Proyecto

```
proyecto/
{chr(10).join([f"â”œâ”€â”€ {archivo}" for archivo in sorted(list(self.cache_archivos.keys())[:15])])}
```

## ğŸŒ Endpoints de la API

### Resumen por Router

"""
            
            for router, endpoints in sorted(routers_map.items()):
                readme += f"\n#### ğŸ“¦ {router}\n\n"
                readme += f"Total de endpoints: **{len(endpoints)}**\n\n"
                readme += "| MÃ©todo | Endpoint | DescripciÃ³n |\n"
                readme += "|--------|----------|-------------|\n"
                
                for ep in sorted(endpoints, key=lambda x: x['endpoint']):
                    metodo = ep['metodo']
                    endpoint = ep['endpoint']
                    desc = ep.get('descripcion', 'Sin descripciÃ³n')[:50]
                    readme += f"| `{metodo}` | `{endpoint}` | {desc} |\n"
                
                readme += "\n"
            
            readme += f"""
## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

```bash
# Clonar repositorio
git clone <url-repositorio>

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cp .env.example .env

# Ejecutar servidor
uvicorn main:app --reload
```

## ğŸ“– Uso de la API

### AutenticaciÃ³n

La API utiliza autenticaciÃ³n basada en tokens JWT. Para obtener un token:

```bash
curl -X POST "http://localhost:8000/auth/login" \\
  -H "Content-Type: application/json" \\
  -d '{{"username": "user", "password": "pass"}}'
```

### Ejemplo de PeticiÃ³n

```python
import requests

response = requests.get(
    "http://localhost:8000/api/endpoint",
    headers={{"Authorization": "Bearer <token>"}}
)
print(response.json())
```

## ğŸ“Š EstadÃ­sticas del Proyecto

- **Total de endpoints:** {len(self.cache_endpoints)}
- **Archivos analizados:** {len(self.cache_archivos)}
- **Routers organizados:** {len(self.cache_routers)}

## ğŸ“ Notas

Esta documentaciÃ³n fue generada automÃ¡ticamente el {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} usando el sistema de documentaciÃ³n con IA.

## ğŸ“„ Licencia

[Especificar licencia]

## ğŸ‘¥ Contribuciones

Las contribuciones son bienvenidas. Por favor, abre un issue o pull request.
"""
            
            # Guardar archivo
            carpeta_path = Path(carpeta_destino) if carpeta_destino else DOCS_OUTPUT_PATH
            carpeta_path.mkdir(parents=True, exist_ok=True)
            
            readme_path = carpeta_path / "README.md"
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(readme)
            
            mensaje = f"""
âœ… **README.md generado exitosamente**

ğŸ“ **UbicaciÃ³n:** `{readme_path}`

ğŸ“Š **Contenido incluido:**
- DescripciÃ³n del proyecto
- {len(self.cache_endpoints)} endpoints documentados
- {len(self.cache_routers)} routers organizados
- Instrucciones de instalaciÃ³n y uso
- Ejemplos de cÃ³digo

ğŸ‰ Â¡Tu documentaciÃ³n estÃ¡ lista! Puedes abrirla y personalizarla segÃºn necesites.
"""
            
            return mensaje, str(readme_path)
            
        except Exception as e:
            return f"âŒ Error generando README: {str(e)}", ""


def crear_prompt_conversacional_v4(
    pregunta: str,
    tipo_pregunta: str,
    codigo: Optional[List[Dict]],
    endpoints: Optional[List[Dict]],
    contexto_tecnico: str,
    historial: List[Dict],
    proyecto_nombre: str
) -> str:
    """
    âœ… NUEVO: Prompt conversacional y natural
    """
    
    # Historial
    historial_texto = ""
    if historial:
        historial_texto = "\n**ğŸ“œ CONTEXTO DE LA CONVERSACIÃ“N:**\n"
        for h in historial[-3:]:
            historial_texto += f"ğŸ‘¤: {h['pregunta'][:80]}...\n"
            historial_texto += f"ğŸ¤–: {h['respuesta'][:120]}...\n\n"
    
    # CASO: Pregunta de lista
    if tipo_pregunta == 'LISTA' and endpoints:
        endpoints_texto = "\n".join([
            f"   â€¢ {ep['metodo']:7} {ep['endpoint']:40} â†’ {ep.get('descripcion', '')[:50]}"
            for ep in endpoints
        ])
        
        return f"""Eres un ingeniero senior experto en el proyecto "{proyecto_nombre}". 
Respondes de forma conversacional, profesional y amigable.

{contexto_tecnico}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ENDPOINTS DISPONIBLES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{endpoints_texto}

Total: {len(endpoints)} endpoints

{historial_texto}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ‘¤ PREGUNTA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{pregunta}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ INSTRUCCIONES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Responde de forma CONVERSACIONAL, como si fueras un compaÃ±ero de equipo
2. Agrupa los endpoints por funcionalidad o router
3. Explica brevemente quÃ© hace cada grupo
4. Menciona los mÃ¡s importantes o interesantes
5. Usa emojis y formato claro
6. NO seas robÃ³tico ni lista todos sin contexto

ğŸ¤– TU RESPUESTA:"""
    
    # CASO: Pregunta de cÃ³digo
    elif tipo_pregunta == 'CODIGO' and codigo:
        codigo_texto = ""
        for i, c in enumerate(codigo[:3], 1):
            codigo_texto += f"""
---
**Fragmento #{i}** (Relevancia: {c['score']:.1%})
ğŸ“„ Archivo: `{c['archivo']}`
ğŸ“Œ Elemento: `{c['elemento']}`
{f"ğŸŒ Endpoint: `{c['metodo_http']} {c['endpoint']}`" if c['endpoint'] else ""}

```python
{c['contenido'][:500]}
```
"""
        
        return f"""Eres un ingeniero senior experto en el proyecto "{proyecto_nombre}".
Respondes de forma conversacional, clara y pedagÃ³gica.

{contexto_tecnico}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CÃ“DIGO RELEVANTE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{codigo_texto}

{historial_texto}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ‘¤ PREGUNTA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{pregunta}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ INSTRUCCIONES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Responde como si explicaras a un colega
2. Usa el CÃ“DIGO que ves arriba para responder
3. Explica el "por quÃ©" y el "cÃ³mo"
4. Menciona archivos y funciones especÃ­ficas
5. SÃ© claro pero conversacional
6. Si no hay cÃ³digo relevante, di que necesitas mÃ¡s contexto

ğŸ¤– TU RESPUESTA:"""
    
    # CASO: Pregunta general
    else:
        return f"""Eres un ingeniero senior experto en el proyecto "{proyecto_nombre}".
Respondes de forma conversacional, profesional y Ãºtil.

{contexto_tecnico}

{historial_texto}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ‘¤ PREGUNTA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{pregunta}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ INSTRUCCIONES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Responde basÃ¡ndote en lo que conoces del proyecto
2. SÃ© conversacional y natural
3. Ofrece ejemplos concretos cuando sea posible
4. Si necesitas mÃ¡s informaciÃ³n, pÃ­dela claramente
5. MantÃ©n un tono profesional pero amigable

ğŸ¤– TU RESPUESTA:"""


def crear_interfaz():
    """Interfaz Gradio V4"""
    
    system = DocumentationSystemV4()
    modelos = list(system.clients.keys()) or ["GPT", "Claude", "DeepSeek"]
    
    css = """
    .main-title {
        text-align: center; 
        padding: 25px; 
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
        border-radius: 12px; 
        color: white;
        font-size: 28px;
        font-weight: bold;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .chat-container {
        border: 2px solid #3498db; 
        border-radius: 12px; 
        padding: 15px;
        background: #f8f9fa;
    }
    .info-box {
        background: linear-gradient(135deg, #667eea15 0%, #764ba215 100%);
        padding: 20px; 
        border-radius: 10px; 
        margin: 15px 0;
        border-left: 5px solid #667eea;
    }
    .success-box {
        background: #d4edda;
        border-left: 5px solid #28a745;
        padding: 15px;
        border-radius: 8px;
        margin: 10px 0;
    }
    """
    
    with gr.Blocks(css=css, title="Agente Conversacional V4") as interface:
        gr.Markdown("""
        # ğŸ¤– Asistente Inteligente de DocumentaciÃ³n
        ### Tu experto conversacional en arquitectura de cÃ³digo
        """, elem_classes=["main-title"])
        
        with gr.Row():
            # CHAT PRINCIPAL
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ Chat Conversacional")
                
                modelo_ia = gr.Dropdown(
                    choices=modelos,
                    label="ğŸ¤– Modelo de IA",
                    value=modelos[0] if modelos else "GPT",
                    interactive=True
                )
                
                chatbot = gr.Chatbot(
                    label="ConversaciÃ³n",
                    height=550,
                    elem_classes=["chat-container"],
                    show_copy_button=True
                )
                
                with gr.Row():
                    mensaje_input = gr.Textbox(
                        placeholder="ğŸ’¬ Escribe tu pregunta aquÃ­... (Ej: Hola, Â¿quÃ© endpoints tienes?)",
                        lines=2,
                        scale=5,
                        label=""
                    )
                    enviar_btn = gr.Button("ğŸ“¤ Enviar", scale=1, variant="primary", size="lg")
                
                limpiar_btn = gr.Button("ğŸ—‘ï¸ Nueva conversaciÃ³n", variant="secondary")
            
            # PANEL LATERAL
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“„ Generar DocumentaciÃ³n")
                
                with gr.Group():
                    carpeta_output = gr.Textbox(
                        label="ğŸ“ Carpeta de destino",
                        value=str(DOCS_OUTPUT_PATH),
                        placeholder="Ruta donde guardar el README.md"
                    )
                    
                    generar_readme_btn = gr.Button(
                        "ğŸ“ Generar README.md",
                        variant="primary",
                        size="lg"
                    )
                    
                    resultado_readme = gr.Markdown(
                        "",
                        elem_classes=["success-box"]
                    )
                
                gr.Markdown(f"""
                <div class="info-box">
                
                ### ğŸ’¡ Sugerencias de ConversaciÃ³n
                
                **ğŸ¯ Para empezar:**
                - *"Hola, Â¿quÃ© puedes hacer?"*
                - *"CuÃ©ntame sobre el proyecto"*
                
                **ğŸ“‹ Explorar endpoints:**
                - *"Â¿QuÃ© endpoints tiene la API?"*
                - *"MuÃ©strame las rutas de autenticaciÃ³n"*
                
                **ğŸ” CÃ³digo especÃ­fico:**
                - *"Â¿CÃ³mo funciona el login?"*
                - *"ExplÃ­came el endpoint /users"*
                
                **ğŸ“š DocumentaciÃ³n:**
                - *"Genera un README completo"*
                - *"Documenta la estructura del proyecto"*
                
                ---
                
                ### ğŸ“Š Estado del Proyecto
                
                - **Endpoints:** {len(system.cache_endpoints)}
                - **Archivos:** {len(system.cache_archivos)}
                - **Routers:** {len(system.cache_routers)}
                
                </div>
                """)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # EVENTOS Y LÃ“GICA
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        def chat_handler(mensaje, historial, modelo):
            """Manejador principal del chat conversacional"""
            
            if not mensaje.strip():
                return historial, ""
            
            if modelo not in system.clients:
                historial.append((mensaje, f"âŒ El modelo {modelo} no estÃ¡ disponible. Verifica tu configuraciÃ³n."))
                return historial, ""
            
            try:
                # âœ… SALUDO INICIAL (primera interacciÃ³n)
                if not system.saludo_inicial_enviado and not historial:
                    system.saludo_inicial_enviado = True
                    saludo = system.generar_saludo_inicial()
                    historial.append((mensaje, saludo))
                    
                    # Guardar en historial
                    system.conversation_history.append({
                        'pregunta': mensaje,
                        'respuesta': saludo
                    })
                    
                    return historial, ""
                
                # âœ… DETECTAR TIPO DE PREGUNTA
                tipo_pregunta = system.detectar_tipo_pregunta(mensaje)
                print(f"\nğŸ¯ Tipo detectado: {tipo_pregunta}")
                
                # âœ… CASO: Generar documentaciÃ³n
                if tipo_pregunta == 'GENERAR_DOC':
                    respuesta = """
Â¡Perfecto! Voy a generar un README.md completo y profesional para tu proyecto. 

ğŸ“ Usa el botÃ³n **"Generar README.md"** en el panel lateral, o si prefieres puedo ayudarte con documentaciÃ³n mÃ¡s especÃ­fica.

Â¿QuÃ© te gustarÃ­a documentar exactamente?
- ğŸ“„ README completo del proyecto
- ğŸŒ DocumentaciÃ³n especÃ­fica de endpoints
- ğŸ”§ GuÃ­as de implementaciÃ³n
- ğŸ“š DocumentaciÃ³n tÃ©cnica detallada
"""
                    historial.append((mensaje, respuesta))
                    system.conversation_history.append({
                        'pregunta': mensaje,
                        'respuesta': respuesta
                    })
                    return historial, ""
                
                # âœ… CASO: Lista de endpoints
                elif tipo_pregunta == 'LISTA':
                    endpoints = system.obtener_todos_endpoints()
                    codigo = None
                    print(f"ğŸ“‹ BÃºsqueda directa: {len(endpoints)} endpoints")
                
                # âœ… CASO: CÃ³digo especÃ­fico
                elif tipo_pregunta == 'CODIGO':
                    codigo = system.buscar_codigo_semantico(mensaje, top_k=5)
                    endpoints = None
                    print(f"ğŸ” BÃºsqueda semÃ¡ntica: {len(codigo)} resultados")
                
                # âœ… CASO: General
                else:
                    codigo = system.buscar_codigo_semantico(mensaje, top_k=3)
                    endpoints = None
                    print(f"ğŸ’¬ BÃºsqueda general")
                
                # Generar contexto tÃ©cnico
                contexto_tecnico = system.generar_contexto_tecnico()
                
                # Crear prompt conversacional
                prompt = crear_prompt_conversacional_v4(
                    mensaje,
                    tipo_pregunta,
                    codigo,
                    endpoints,
                    contexto_tecnico,
                    system.conversation_history,
                    system.proyecto_nombre
                )
                
                # âœ… LLAMAR AL MODELO DE IA
                print(f"ğŸ¤– Generando respuesta con {modelo}...")
                
                if modelo == "GPT":
                    response = system.clients["GPT"].chat.completions.create(
                        model=MODEL_CONFIG["GPT"]["model"],
                        messages=[
                            {"role": "system", "content": "Eres un ingeniero senior experto y conversacional."},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.4,
                        max_tokens=2500
                    )
                    respuesta = response.choices[0].message.content
                
                elif modelo == "Claude":
                    response = system.clients["Claude"].messages.create(
                        model=MODEL_CONFIG["Claude"]["model"],
                        max_tokens=2500,
                        temperature=0.4,
                        messages=[{"role": "user", "content": prompt}]
                    )
                    respuesta = response.content[0].text
                
                elif modelo == "DeepSeek":
                    response = system.clients["DeepSeek"].chat.completions.create(
                        model=MODEL_CONFIG["DeepSeek"]["model"],
                        messages=[
                            {"role": "system", "content": "Eres un ingeniero senior experto y conversacional."},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.4,
                        max_tokens=2500
                    )
                    respuesta = response.choices[0].message.content
                
                # Guardar en historial
                system.conversation_history.append({
                    'pregunta': mensaje,
                    'respuesta': respuesta
                })
                
                historial.append((mensaje, respuesta))
                print("âœ… Respuesta generada")
                
                return historial, ""
            
            except Exception as e:
                print(f"âŒ Error: {e}")
                import traceback
                traceback.print_exc()
                
                error_msg = f"""
âŒ **Ups, hubo un error...**

{str(e)}

Intenta reformular tu pregunta o verifica que:
- âœ… El modelo de IA estÃ© correctamente configurado
- âœ… Tengas conexiÃ³n a internet
- âœ… Las API keys sean vÃ¡lidas
"""
                historial.append((mensaje, error_msg))
                return historial, ""
        
        def generar_readme_handler(carpeta):
            """Manejador para generar README"""
            try:
                mensaje, path = system.generar_readme_completo(carpeta)
                return mensaje
            except Exception as e:
                return f"âŒ Error: {str(e)}"
        
        def limpiar_chat():
            """Limpia el chat y reinicia la conversaciÃ³n"""
            system.conversation_history = []
            system.saludo_inicial_enviado = False
            return [], ""
        
        # Conectar eventos
        enviar_btn.click(
            fn=chat_handler,
            inputs=[mensaje_input, chatbot, modelo_ia],
            outputs=[chatbot, mensaje_input]
        )
        
        mensaje_input.submit(
            fn=chat_handler,
            inputs=[mensaje_input, chatbot, modelo_ia],
            outputs=[chatbot, mensaje_input]
        )
        
        generar_readme_btn.click(
            fn=generar_readme_handler,
            inputs=[carpeta_output],
            outputs=[resultado_readme]
        )
        
        limpiar_btn.click(
            fn=limpiar_chat,
            inputs=[],
            outputs=[chatbot, mensaje_input]
        )
    
    return interface


if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸš€ AGENTE DE DOCUMENTACIÃ“N V4 - CONVERSACIONAL")
    print("="*80 + "\n")
    
    print("âœ… Saludo natural y contextual")
    print("âœ… Personalidad conversacional como experto")
    print("âœ… GeneraciÃ³n de README.md profesional")
    print("âœ… BÃºsqueda inteligente (directa + semÃ¡ntica)")
    print("âœ… InteracciÃ³n fluida y natural\n")
    
    interface = crear_interfaz()
    interface.launch(
        server_name="localhost",
        server_port=7865,
        share=False,
        show_error=True
    )